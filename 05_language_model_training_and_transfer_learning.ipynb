{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model Training and Transfer Learning\n",
    "This notebook shows how to create and train a language model as well as how to reuse parts of it for a new task.  In our case we will try the IMDB task again, as well as a new task which attempts to classify the ratings of Rotten Tomatoes reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, json, pickle\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, vstack, lil_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training Data\n",
    "\n",
    "We set up on Amazon review data in this section.  The review gets tokenized and turned into an integer representation, and then saved to an HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('vocab_count.pkl', 'rb') as f:\n",
    "    vocab_count = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_LIMIT = 100000\n",
    "VOCAB_FREQ_MIN = 2\n",
    "MAX_LEN = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_list = [o for o, c in vocab_count.most_common(VOCAB_LIMIT-2) if c > VOCAB_FREQ_MIN]\n",
    "vocab_list.insert(0, '<unk>')\n",
    "vocab_list.insert(0, '<pad>')\n",
    "int2word_hash = {i:w for i, w in enumerate(vocab_list)}\n",
    "word2int_hash = {v:k for k, v in int2word_hash.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/amazon/movie_review_tokenized.json', 'r') as in_file:\n",
    "    completed_reviews = 0\n",
    "    for line in in_file:\n",
    "        if completed_reviews > 500000:\n",
    "            break\n",
    "        review = json.loads(line)\n",
    "        review_text = review['reviewTextTokenized']\n",
    "        hashed_review = [word2int_hash.get(x,1) if word2int_hash.get(x,1) < VOCAB_LIMIT-2 else 1 for x in review_text]\n",
    "        h5_file = h5py.File('data/language_pkl/amazon_language_data.h5','a')\n",
    "        h5_file.create_dataset('amazon_lm_'+str(completed_reviews), data=hashed_review)\n",
    "        h5_file.close()\n",
    "        completed_reviews += 1\n",
    "        if completed_reviews % 10000 == 0:\n",
    "            print('Completed: {}'.format(completed_reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Word Embeddings\n",
    "\n",
    "In this step, we load the word embeddings created in the previous notebook using the IMDB and Amazon data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wv_model = Word2Vec.load('data/w2v_192_language_model_tokens_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_embedding_matrix(wv_model, index_word_dict, max_len=None, verbose=False):\n",
    "    EMBED_SIZE = wv_model.vector_size\n",
    "    unknown_word_count = 0\n",
    "    unknown_word_list = list()\n",
    "    \n",
    "    def choose_embedded_vector(wv_model, word, unknown_word_count, verbose=verbose):\n",
    "        if word in wv_model.wv.vocab:\n",
    "            return wv_model.wv.word_vec(word), unknown_word_count\n",
    "        else:\n",
    "            if verbose:\n",
    "                unknown_word_list.append(word)\n",
    "            return np.random.uniform(low=-.05, high=.05, size=EMBED_SIZE), (unknown_word_count+1)\n",
    "\n",
    "    word_index_dict = {v:k for k, v in index_word_dict.items()}\n",
    "    num_words = len(index_word_dict)\n",
    "\n",
    "\n",
    "    embedding_weights = np.zeros((num_words, EMBED_SIZE))\n",
    "    for word, index in word_index_dict.items():\n",
    "        embedding_weights[index,:], unknown_word_count = choose_embedded_vector(wv_model, word, unknown_word_count, verbose=verbose)\n",
    "    \n",
    "    if verbose:\n",
    "        print('The size of the data token vocab is: {}'.format(num_words))\n",
    "        print('The size of the word vector vocab is: {}'.format(EMBED_SIZE))\n",
    "        print('Embedding matrix shape: {}'.format(embedding_weights.shape))\n",
    "        print('Total amount of words not found in gensim word2vec model: {}'.format(unknown_word_count))\n",
    "        print('The words not found in gensim word2vec model: {}'.format(str(unknown_word_list)))\n",
    "        \n",
    "    \n",
    "    return embedding_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the data token vocab is: 100000\n",
      "The size of the word vector vocab is: 192\n",
      "Embedding matrix shape: (100000, 192)\n",
      "Total amount of words not found in gensim word2vec model: 2\n",
      "The words not found in gensim word2vec model: ['<pad>', '<unk>']\n"
     ]
    }
   ],
   "source": [
    "embedding_weights = create_embedding_matrix(wv_model, int2word_hash, max_len=None, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Batch Generator\n",
    "\n",
    "Because our data is too big to put in memory, we have to read it in batches from disk.  Keras has a special type of fitting function called fit_generator that allows us to pass in a generator instead of an object in memory.  This section creates the generator that eventually yields the X and y for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "from scipy.sparse import vstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_fwd_target_matrix(s):\n",
    "    rolled_review = np.roll(s, -1)\n",
    "    rolled_review[-1] = 0\n",
    "    tmp = lil_matrix((rolled_review.shape[0], VOCAB_LIMIT))\n",
    "    for i, w in enumerate(s):\n",
    "        tmp[i,w] = 1\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_generator(hdf5_file_location, batch_size):\n",
    "    hf = h5py.File(hdf5_file_location, 'r')\n",
    "    hf_dataset_names = list(hf.keys())\n",
    "    \n",
    "    samples_per_epoch = len(hf_dataset_names)\n",
    "    number_of_batches = int(samples_per_epoch / batch_size)\n",
    "    counter = 0\n",
    "    \n",
    "    shuffle(hf_dataset_names)\n",
    "    \n",
    "    while True:\n",
    "        index_batch = hf_dataset_names[batch_size*counter:batch_size*(counter+1)]\n",
    "        batch_list = list()\n",
    "        for f in index_batch:\n",
    "            batch_list.append(pad_sequences([hf.get(f).value], maxlen=MAX_LEN)[0])\n",
    "        X_batch = np.vstack(batch_list)\n",
    "        y_forward_batch = np.reshape(np.array(np.vstack([create_fwd_target_matrix(x).todense() for x in X_batch])), (batch_size, MAX_LEN, -1))\n",
    "        y_backward_batch = np.flip(np.roll(y_forward_batch, 2, axis=1), axis=1)\n",
    "        counter += 1\n",
    "        yield(X_batch, [y_forward_batch, y_backward_batch])\n",
    "        \n",
    "        if (counter < number_of_batches):\n",
    "            shuffle(hf_dataset_names)\n",
    "            counter=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Language Model\n",
    "\n",
    "This section creates and trains the language model.  Since we have so much data, one single pass through the reviews is sufficient.  Fair waring, this will take about 24-48 hours to train depending on your compute resources and GPU size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model, load_model\n",
    "from keras.regularizers import l2\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.layers import Layer, Input, Embedding, Dropout, Conv1D, concatenate, LSTM, TimeDistributed, Dense, Add, GlobalMaxPool1D, Bidirectional\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras import backend as K "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBED_SIZE = wv_model.vector_size\n",
    "CNN_FILTER_SIZE = 96\n",
    "CNN_KERNEL_SIZE_1 = 3\n",
    "CNN_KERNEL_SIZE_2 = 4\n",
    "CNN_KERNEL_SIZE_3 = 5\n",
    "LSTM_HIDDEN = 64\n",
    "VOCAB_SIZE = len(int2word_hash)\n",
    "\n",
    "def get_language_model():    \n",
    "    input_text = Input(shape=(MAX_LEN, ))\n",
    "    x = Embedding(input_dim=VOCAB_SIZE, output_dim=EMBED_SIZE, input_length=MAX_LEN, weights=[embedding_weights], mask_zero=False, trainable=False, name='word_embeddings')(input_text)\n",
    "    x = Dropout(0.5)(x)\n",
    "    c1 = Conv1D(filters=CNN_FILTER_SIZE, kernel_size=CNN_KERNEL_SIZE_1, padding='same', kernel_regularizer=l2(0.001))(x)\n",
    "    c2 = Conv1D(filters=CNN_FILTER_SIZE, kernel_size=CNN_KERNEL_SIZE_2, padding='same', kernel_regularizer=l2(0.001))(x)\n",
    "    c3 = Conv1D(filters=CNN_FILTER_SIZE, kernel_size=CNN_KERNEL_SIZE_3, padding='same', kernel_regularizer=l2(0.001))(x)\n",
    "    x = concatenate([c1,c2,c3])\n",
    "    \n",
    "    l_f = LSTM(LSTM_HIDDEN,\n",
    "               return_sequences=True,\n",
    "               return_state=False,\n",
    "               kernel_regularizer=l2(0.001),\n",
    "               recurrent_regularizer=l2(0.001))(x)\n",
    "    \n",
    "    output_f = TimeDistributed(Dense(VOCAB_SIZE, activation=\"softmax\"))(l_f)\n",
    "    \n",
    "    l_b = LSTM(LSTM_HIDDEN,\n",
    "               return_sequences=True,\n",
    "               return_state=False,\n",
    "               kernel_regularizer=l2(0.001),\n",
    "               recurrent_regularizer=l2(0.001),\n",
    "               go_backwards=True)(x)\n",
    "    \n",
    "    output_b = TimeDistributed(Dense(VOCAB_SIZE, activation=\"softmax\"))(l_b)\n",
    "    \n",
    "    opt = Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    \n",
    "    model = Model(inputs=[input_text], outputs=[output_f, output_b])\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = get_language_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "word_embeddings (Embedding)     (None, 250, 192)     19200000    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 250, 192)     0           word_embeddings[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 250, 96)      55392       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 250, 96)      73824       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 250, 96)      73824       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 250, 288)     0           conv1d_1[0][0]                   \n",
      "                                                                 conv1d_2[0][0]                   \n",
      "                                                                 conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 250, 64)      90368       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 250, 64)      90368       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 250, 100000)  6500000     lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 250, 100000)  6500000     lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 32,583,776\n",
      "Trainable params: 13,383,776\n",
      "Non-trainable params: 19,200,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hf = h5py.File('data/language_pkl/amazon_language_data.h5', 'r')\n",
    "hf_dataset_names = list(hf.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "EPOCHS = 1\n",
    "STEPS_PER_EPOCH = int(len(hf_dataset_names) / BATCH_SIZE)\n",
    "VAL_STEPS = int(len(hf_dataset_names) / BATCH_SIZE)\n",
    "MODEL_NAME = 'language_model_bidirectional'\n",
    "FILE_PATH = \"../models/language_model_bidirectional.hdf5\"\n",
    "checkpoint = ModelCheckpoint(FILE_PATH, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=2)\n",
    "tensorboard = TensorBoard(log_dir='/home/ml-notebooks/models/tensorboard/{}'.format(MODEL_NAME), \n",
    "                          batch_size=BATCH_SIZE)\n",
    "callbacks_list = [checkpoint, early]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " 9676/31060 [========>.....................] - ETA: 17:16:27 - loss: 4.8825 - time_distributed_1_loss: 1.5027 - time_distributed_2_loss: 3.2291 - time_distributed_1_acc: 0.8080 - time_distributed_2_acc: 0.5425"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-5a1b48e08b84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit_generator(generator = batch_generator('data/language_pkl/amazon_language_data.h5', BATCH_SIZE),\n\u001b[1;32m      2\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSTEPS_PER_EPOCH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                     epochs = EPOCHS)\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2175\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2176\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1847\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1849\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit_generator(generator = batch_generator('data/language_pkl/amazon_language_data.h5', BATCH_SIZE),\n",
    "                    steps_per_epoch = STEPS_PER_EPOCH,\n",
    "                    epochs = EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('data/lm_amazon_reviews_250k.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_review = [\"<sos>\", \"this\", \"is\", \"a\", \"charming\", \"version\", \"of\", \"the\", \"classic\", \"dicken\", \"'s\", \"tale\", \".\", \"henry\", \"winkler\", \"makes\", \"a\", \"good\", \"showing\", \"as\", \"the\", \"scrooge\", \"character\", \".\", \"even\", \"though\", \"you\", \"know\", \"what\", \"will\", \"happen\", \"this\", \"version\", \"has\", \"enough\", \"of\", \"a\", \"change\", \"to\", \"make\", \"it\", \"better\", \"that\", \"average\", \".\", \"if\", \"you\", \"love\", \"a\", \"christmas\", \"carol\", \"in\", \"any\", \"version\", \",\", \"then\", \"you\", \"will\", \"love\", \"this\", \".\", \"<eos>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<sos> this is a charming version of the classic dicken 's tale . henry winkler makes a good showing as the scrooge character . even though you know what will happen this version has enough of a change to make it better that average . if you love a christmas carol in any version , then you will love this . <eos>\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(text_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_review_hashed = [word2int_hash.get(x,1) for x in text_review]\n",
    "text_review_pad = pad_sequences([text_review_hashed], maxlen=MAX_LEN)\n",
    "text_hat = model.predict(text_review_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<sos> this is a charming version of the classic disney 's tale . henry nicholson makes a good showing as the character . even though you know what will happen this version has enough of a change to make it better that average . if you love a christmas carol in any version , then you will love this . <eos>\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([int2word_hash.get(x,0) for x in np.argmax(text_hat[0][0], axis=1) if x != 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning: Prep Classification Data\n",
    "\n",
    "In this section we prepare our data for training a new sentiment model.  We prep both the IMDB and Rotten Tomatoes data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 3)\n",
      "(25000, 3)\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA_FOLDER = 'data/aclImdb/train/'\n",
    "TEST_DATA_FOLDER = 'data/aclImdb/test/'\n",
    "\n",
    "def create_dataframe_from_files(data_folder):\n",
    "    examples = list()\n",
    "    for d in ['pos','neg']:\n",
    "        for f in os.listdir(os.path.join(data_folder,d)):\n",
    "            _tmp = open(os.path.join(data_folder,d,f),'r', encoding='utf-8')\n",
    "            if d=='pos':\n",
    "                examples += [(_tmp.read(),f,1)]\n",
    "            else:\n",
    "                examples += [(_tmp.read(),f,0)]\n",
    "    df_tmp = pd.DataFrame(examples, columns=['text','file','target'])\n",
    "    df_tmp = df_tmp.sample(frac=1)\n",
    "    df_tmp = df_tmp.reset_index(drop=True)\n",
    "    return df_tmp\n",
    "                \n",
    "df_train = create_dataframe_from_files(TRAIN_DATA_FOLDER)\n",
    "df_test = create_dataframe_from_files(TEST_DATA_FOLDER)\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_review(review_text):\n",
    "    review_text = re.sub('[^A-Za-z0-9.?!\\',-/ ]+', '', review_text)\n",
    "    review_text = review_text.replace('.', ' . ').replace(',', ' , ').replace('-', ' - ').replace('/', ' / ').replace('!', ' ! ').replace('?', ' ? ').replace('\\'s', ' \\'s ').lower()\n",
    "    review_text = ['<sos>'] + review_text.split(' ') + ['<eos>']\n",
    "    review_text = [x for x in review_text if len(x) > 0]\n",
    "    return review_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train['tokenized_text'] = df_train['text'].apply(lambda x: tokenize_review(x))\n",
    "df_test['tokenized_text'] = df_test['text'].apply(lambda x: tokenize_review(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_sequence_train = list()\n",
    "\n",
    "for item in df_train['tokenized_text'].values:\n",
    "    tokenized_sequence_train.append([word2int_hash.get(x,1) for x in item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_sequence_test = list()\n",
    "\n",
    "for item in df_test['tokenized_text'].values:\n",
    "    tokenized_sequence_test.append([word2int_hash.get(x,1) for x in item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed=42)\n",
    "train_index = np.random.choice(range(0,len(tokenized_sequence_train)), size=int(0.95*len(tokenized_sequence_train)), replace=False)\n",
    "val_index = np.setdiff1d(range(0, len(tokenized_sequence_train)), train_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23750, 250)\n",
      "(1250, 250)\n",
      "(25000, 250)\n",
      "(23750, 2)\n",
      "(1250, 2)\n",
      "(25000, 2)\n"
     ]
    }
   ],
   "source": [
    "X = pad_sequences(tokenized_sequence_train, maxlen=MAX_LEN)\n",
    "X_train = X[train_index]\n",
    "X_val = X[val_index]\n",
    "X_test = pad_sequences(tokenized_sequence_test, maxlen=MAX_LEN)\n",
    "y = np.vstack(df_train['target'].apply(lambda x: np.array([0,1]) if x == 1 else np.array([1,0])).values)\n",
    "y_train = y[train_index]\n",
    "y_val = y[val_index]\n",
    "y_test = np.vstack(df_test['target'].apply(lambda x: np.array([0,1]) if x == 1 else np.array([1,0])).values)\n",
    "y_test_rand = np.vstack(df_test['target'].apply(lambda x: np.array([0,1]) if np.random.choice([0,1], p=[df_train['target'].mean(), 1-df_train['target'].mean()]) == 1 else np.array([1,0])).values)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rotten Tomatoes Kaggle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PhraseId', 'SentenceId', 'Phrase', 'Sentiment'], dtype='object')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kaggle_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kaggle_sentiment_to_vector(sentiment):\n",
    "    if sentiment == 0:\n",
    "        return np.array([1,0,0,0,0])\n",
    "    elif sentiment == 1:\n",
    "        return np.array([0,1,0,0,0])\n",
    "    elif sentiment == 2:\n",
    "        return np.array([0,0,1,0,0])\n",
    "    elif sentiment == 3:\n",
    "        return np.array([0,0,0,1,0])\n",
    "    elif sentiment == 4:\n",
    "        return np.array([0,0,0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148257, 250)\n",
      "(7803, 250)\n",
      "(66292, 250)\n",
      "(148257, 5)\n",
      "(7803, 5)\n"
     ]
    }
   ],
   "source": [
    "df_kaggle_train = pd.read_csv('data/kaggle_rotten_tomatoes/train.tsv', sep='\\t')\n",
    "df_kaggle_test = pd.read_csv('data/kaggle_rotten_tomatoes/test.tsv', sep='\\t')\n",
    "df_kaggle_train['tokenized_text'] = df_kaggle_train['Phrase'].apply(lambda x: tokenize_review(x))\n",
    "df_kaggle_test['tokenized_text'] = df_kaggle_test['Phrase'].apply(lambda x: tokenize_review(x))\n",
    "\n",
    "kaggle_tokenized_sequence_train = list()\n",
    "for item in df_kaggle_train['tokenized_text'].values:\n",
    "    kaggle_tokenized_sequence_train.append([word2int_hash.get(x,1) for x in item])\n",
    "    \n",
    "kaggle_tokenized_sequence_test = list()\n",
    "for item in df_kaggle_test['tokenized_text'].values:\n",
    "    kaggle_tokenized_sequence_test.append([word2int_hash.get(x,1) for x in item])\n",
    "\n",
    "    \n",
    "np.random.seed(seed=42)\n",
    "kaggle_train_index = np.random.choice(range(0,len(kaggle_tokenized_sequence_train)), size=int(0.95*len(kaggle_tokenized_sequence_train)), replace=False)\n",
    "kaggle_val_index = np.setdiff1d(range(0, len(kaggle_tokenized_sequence_train)), kaggle_train_index)\n",
    "    \n",
    "    \n",
    "X_kaggle = pad_sequences(kaggle_tokenized_sequence_train, maxlen=MAX_LEN)\n",
    "X_kaggle_train = X_kaggle[kaggle_train_index]\n",
    "X_kaggle_val = X_kaggle[kaggle_val_index]\n",
    "X_kaggle_test = pad_sequences(kaggle_tokenized_sequence_test, maxlen=MAX_LEN)\n",
    "y_kaggle = np.vstack(df_kaggle_train['Sentiment'].apply(lambda x: kaggle_sentiment_to_vector(x)).values)\n",
    "y_kaggle_train = y_kaggle[kaggle_train_index]\n",
    "y_kaggle_val = y_kaggle[kaggle_val_index]\n",
    "\n",
    "print(X_kaggle_train.shape)\n",
    "print(X_kaggle_val.shape)\n",
    "print(X_kaggle_test.shape)\n",
    "print(y_kaggle_train.shape)\n",
    "print(y_kaggle_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning: Create Classification Model\n",
    "In this section, we create a new model that takes a portion of our newly trained language model, and then glues on a sentiment classification task.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LANGUAGE_FILE_PATH = 'data/lm_amazon_reviews_250k.hdf5'\n",
    "language_model = load_model(LANGUAGE_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x7f90242a8b38>,\n",
       " <keras.layers.embeddings.Embedding at 0x7f90242a8b00>,\n",
       " <keras.layers.core.Dropout at 0x7f90242a89e8>,\n",
       " <keras.layers.convolutional.Conv1D at 0x7f90242a8ac8>,\n",
       " <keras.layers.convolutional.Conv1D at 0x7f90242a89b0>,\n",
       " <keras.layers.convolutional.Conv1D at 0x7f90242a8400>,\n",
       " <keras.layers.merge.Concatenate at 0x7f90242a8160>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_model.layers.pop()\n",
    "language_model.layers.pop()\n",
    "language_model.layers.pop()\n",
    "language_model.layers.pop()\n",
    "language_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBED_SIZE = wv_model.vector_size\n",
    "CNN_FILTER_SIZE = 96\n",
    "CNN_KERNEL_SIZE_1 = 3\n",
    "CNN_KERNEL_SIZE_2 = 4\n",
    "CNN_KERNEL_SIZE_3 = 5\n",
    "LSTM_HIDDEN = 64\n",
    "VOCAB_SIZE = len(int2word_hash)\n",
    "DENSE_HIDDEN = 128\n",
    "N_CLASSES = 2\n",
    "\n",
    "def get_model(language_model=None, freeze_before_layer=None):\n",
    "    if language_model:\n",
    "        input_text = language_model.input\n",
    "        x = Bidirectional(LSTM(LSTM_HIDDEN,\n",
    "                               return_sequences=True,\n",
    "                               kernel_regularizer=l2(0.001),\n",
    "                               recurrent_regularizer=l2(0.001)))(language_model.layers[-1].output)\n",
    "    else:\n",
    "        input_text = Input(shape=(MAX_LEN, ))\n",
    "        x = Embedding(input_dim=VOCAB_SIZE, output_dim=EMBED_SIZE, input_length=MAX_LEN, weights=[embedding_weights], mask_zero=False, trainable=False, name='word_embeddings')(input_text)\n",
    "        #x = Embedding(input_dim=VOCAB_SIZE, output_dim=EMBED_SIZE, input_length=MAX_LEN, mask_zero=False, trainable=True, name='word_embeddings')(input_text)\n",
    "        x = Dropout(0.5)(x)\n",
    "        c1 = Conv1D(filters=CNN_FILTER_SIZE, kernel_size=CNN_KERNEL_SIZE_1, padding='same', kernel_regularizer=l2(0.001))(x)\n",
    "        c2 = Conv1D(filters=CNN_FILTER_SIZE, kernel_size=CNN_KERNEL_SIZE_2, padding='same', kernel_regularizer=l2(0.001))(x)\n",
    "        c3 = Conv1D(filters=CNN_FILTER_SIZE, kernel_size=CNN_KERNEL_SIZE_3, padding='same', kernel_regularizer=l2(0.001))(x)\n",
    "        x = concatenate([c1,c2,c3])\n",
    "        x = Bidirectional(LSTM(LSTM_HIDDEN,\n",
    "                       return_sequences=True,\n",
    "                       kernel_regularizer=l2(0.001),\n",
    "                       recurrent_regularizer=l2(0.001)))(x)\n",
    "    \n",
    "    x = AttentionWithContext()(x)\n",
    "    x = Dense(DENSE_HIDDEN, activation=\"elu\", kernel_regularizer=l2(0.001))(x)\n",
    "    output_s = Dense(N_CLASSES, activation=\"softmax\")(x)\n",
    "    \n",
    "    opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    #opt = SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "    \n",
    "    model = Model(inputs=[input_text], outputs=[output_s])\n",
    "    \n",
    "    if freeze_before_layer:\n",
    "        for layer in model.layers[:freeze_before_layer]:\n",
    "            layer.trainable = False\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = get_model(language_model=language_model, freeze_before_layer=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "word_embeddings (Embedding)     (None, 250, 192)     19200000    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 250, 192)     0           word_embeddings[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 250, 96)      55392       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 250, 96)      73824       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 250, 96)      73824       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 250, 288)     0           conv1d_1[0][0]                   \n",
      "                                                                 conv1d_2[0][0]                   \n",
      "                                                                 conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 250, 128)     180736      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "attention_with_context_2 (Atten (None, 128)          16640       bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 128)          16512       attention_with_context_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 2)            258         dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 19,617,186\n",
      "Trainable params: 214,146\n",
      "Non-trainable params: 19,403,040\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"702pt\" viewBox=\"0.00 0.00 743.50 702.00\" width=\"744pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 698)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-698 739.5,-698 739.5,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 139733934719832 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>139733934719832</title>\n",
       "<polygon fill=\"none\" points=\"291.5,-657.5 291.5,-693.5 451.5,-693.5 451.5,-657.5 291.5,-657.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"371.5\" y=\"-671.8\">input_1: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139733934719944 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>139733934719944</title>\n",
       "<polygon fill=\"none\" points=\"252.5,-584.5 252.5,-620.5 490.5,-620.5 490.5,-584.5 252.5,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"371.5\" y=\"-598.8\">word_embeddings: Embedding</text>\n",
       "</g>\n",
       "<!-- 139733934719832&#45;&gt;139733934719944 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>139733934719832-&gt;139733934719944</title>\n",
       "<path d=\"M371.5,-657.4551C371.5,-649.3828 371.5,-639.6764 371.5,-630.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"375.0001,-630.5903 371.5,-620.5904 368.0001,-630.5904 375.0001,-630.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139733934720224 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>139733934720224</title>\n",
       "<polygon fill=\"none\" points=\"293,-511.5 293,-547.5 450,-547.5 450,-511.5 293,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"371.5\" y=\"-525.8\">dropout_1: Dropout</text>\n",
       "</g>\n",
       "<!-- 139733934719944&#45;&gt;139733934720224 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>139733934719944-&gt;139733934720224</title>\n",
       "<path d=\"M371.5,-584.4551C371.5,-576.3828 371.5,-566.6764 371.5,-557.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"375.0001,-557.5903 371.5,-547.5904 368.0001,-557.5904 375.0001,-557.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139733934720056 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>139733934720056</title>\n",
       "<polygon fill=\"none\" points=\"127,-438.5 127,-474.5 278,-474.5 278,-438.5 127,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"202.5\" y=\"-452.8\">conv1d_1: Conv1D</text>\n",
       "</g>\n",
       "<!-- 139733934720224&#45;&gt;139733934720056 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>139733934720224-&gt;139733934720056</title>\n",
       "<path d=\"M329.7247,-511.4551C306.7713,-501.5403 278.1102,-489.16 253.8055,-478.6615\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"254.9485,-475.3428 244.3804,-474.5904 252.1727,-481.7689 254.9485,-475.3428\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139733934720280 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>139733934720280</title>\n",
       "<polygon fill=\"none\" points=\"296,-438.5 296,-474.5 447,-474.5 447,-438.5 296,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"371.5\" y=\"-452.8\">conv1d_2: Conv1D</text>\n",
       "</g>\n",
       "<!-- 139733934720224&#45;&gt;139733934720280 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>139733934720224-&gt;139733934720280</title>\n",
       "<path d=\"M371.5,-511.4551C371.5,-503.3828 371.5,-493.6764 371.5,-484.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"375.0001,-484.5903 371.5,-474.5904 368.0001,-484.5904 375.0001,-484.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139733934720672 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>139733934720672</title>\n",
       "<polygon fill=\"none\" points=\"465,-438.5 465,-474.5 616,-474.5 616,-438.5 465,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"540.5\" y=\"-452.8\">conv1d_3: Conv1D</text>\n",
       "</g>\n",
       "<!-- 139733934720224&#45;&gt;139733934720672 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>139733934720224-&gt;139733934720672</title>\n",
       "<path d=\"M413.2753,-511.4551C436.2287,-501.5403 464.8898,-489.16 489.1945,-478.6615\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"490.8273,-481.7689 498.6196,-474.5904 488.0515,-475.3428 490.8273,-481.7689\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139733934721568 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>139733934721568</title>\n",
       "<polygon fill=\"none\" points=\"261,-365.5 261,-401.5 482,-401.5 482,-365.5 261,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"371.5\" y=\"-379.8\">concatenate_1: Concatenate</text>\n",
       "</g>\n",
       "<!-- 139733934720056&#45;&gt;139733934721568 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>139733934720056-&gt;139733934721568</title>\n",
       "<path d=\"M244.2753,-438.4551C267.2287,-428.5403 295.8898,-416.16 320.1945,-405.6615\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"321.8273,-408.7689 329.6196,-401.5904 319.0515,-402.3428 321.8273,-408.7689\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139733934720280&#45;&gt;139733934721568 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>139733934720280-&gt;139733934721568</title>\n",
       "<path d=\"M371.5,-438.4551C371.5,-430.3828 371.5,-420.6764 371.5,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"375.0001,-411.5903 371.5,-401.5904 368.0001,-411.5904 375.0001,-411.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139733934720672&#45;&gt;139733934721568 -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>139733934720672-&gt;139733934721568</title>\n",
       "<path d=\"M498.7247,-438.4551C475.7713,-428.5403 447.1102,-416.16 422.8055,-405.6615\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"423.9485,-402.3428 413.3804,-401.5904 421.1727,-408.7689 423.9485,-402.3428\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139733790101800 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>139733790101800</title>\n",
       "<polygon fill=\"none\" points=\"70.5,-292.5 70.5,-328.5 408.5,-328.5 408.5,-292.5 70.5,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"239.5\" y=\"-306.8\">bidirectional_5(lstm_5): Bidirectional(LSTM)</text>\n",
       "</g>\n",
       "<!-- 139733934721568&#45;&gt;139733790101800 -->\n",
       "<g class=\"edge\" id=\"edge9\">\n",
       "<title>139733934721568-&gt;139733790101800</title>\n",
       "<path d=\"M338.8708,-365.4551C321.4186,-355.8035 299.742,-343.8156 281.0919,-333.5016\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"282.6561,-330.3671 272.2113,-328.5904 279.2684,-336.4928 282.6561,-330.3671\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139733790102696 -->\n",
       "<g class=\"node\" id=\"node9\">\n",
       "<title>139733790102696</title>\n",
       "<polygon fill=\"none\" points=\"454.5,-292.5 454.5,-328.5 608.5,-328.5 608.5,-292.5 454.5,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"531.5\" y=\"-306.8\">skip_conv: Conv1D</text>\n",
       "</g>\n",
       "<!-- 139733934721568&#45;&gt;139733790102696 -->\n",
       "<g class=\"edge\" id=\"edge10\">\n",
       "<title>139733934721568-&gt;139733790102696</title>\n",
       "<path d=\"M411.0506,-365.4551C432.6855,-355.5841 459.6764,-343.2695 482.6211,-332.801\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"484.2049,-335.9255 491.8499,-328.5904 481.2992,-329.5571 484.2049,-335.9255\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139733401093736 -->\n",
       "<g class=\"node\" id=\"node10\">\n",
       "<title>139733401093736</title>\n",
       "<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 367,-255.5 367,-219.5 0,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"183.5\" y=\"-233.8\">attention_with_context_4: AttentionWithContext</text>\n",
       "</g>\n",
       "<!-- 139733790101800&#45;&gt;139733401093736 -->\n",
       "<g class=\"edge\" id=\"edge11\">\n",
       "<title>139733790101800-&gt;139733401093736</title>\n",
       "<path d=\"M225.6573,-292.4551C218.9938,-283.7686 210.8785,-273.1898 203.5541,-263.642\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"206.2412,-261.3944 197.3775,-255.5904 200.6872,-265.655 206.2412,-261.3944\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139733401640408 -->\n",
       "<g class=\"node\" id=\"node11\">\n",
       "<title>139733401640408</title>\n",
       "<polygon fill=\"none\" points=\"385.5,-219.5 385.5,-255.5 735.5,-255.5 735.5,-219.5 385.5,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"560.5\" y=\"-233.8\">global_max_pooling1d_1: GlobalMaxPooling1D</text>\n",
       "</g>\n",
       "<!-- 139733790102696&#45;&gt;139733401640408 -->\n",
       "<g class=\"edge\" id=\"edge12\">\n",
       "<title>139733790102696-&gt;139733401640408</title>\n",
       "<path d=\"M538.6685,-292.4551C541.945,-284.2074 545.8993,-274.2536 549.5371,-265.0962\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"552.8741,-266.1761 553.3134,-255.5904 546.3687,-263.5917 552.8741,-266.1761\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139733401092280 -->\n",
       "<g class=\"node\" id=\"node12\">\n",
       "<title>139733401092280</title>\n",
       "<polygon fill=\"none\" points=\"323,-146.5 323,-182.5 420,-182.5 420,-146.5 323,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"371.5\" y=\"-160.8\">add_3: Add</text>\n",
       "</g>\n",
       "<!-- 139733401093736&#45;&gt;139733401092280 -->\n",
       "<g class=\"edge\" id=\"edge13\">\n",
       "<title>139733401093736-&gt;139733401092280</title>\n",
       "<path d=\"M229.9719,-219.4551C255.8448,-209.4087 288.2369,-196.8309 315.5006,-186.2445\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"316.8561,-189.4728 324.9111,-182.5904 314.3223,-182.9474 316.8561,-189.4728\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139733401640408&#45;&gt;139733401092280 -->\n",
       "<g class=\"edge\" id=\"edge14\">\n",
       "<title>139733401640408-&gt;139733401092280</title>\n",
       "<path d=\"M513.7809,-219.4551C487.7703,-209.4087 455.206,-196.8309 427.7973,-186.2445\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"428.9261,-182.9285 418.3367,-182.5904 426.404,-189.4584 428.9261,-182.9285\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139733772990560 -->\n",
       "<g class=\"node\" id=\"node13\">\n",
       "<title>139733772990560</title>\n",
       "<polygon fill=\"none\" points=\"307.5,-73.5 307.5,-109.5 435.5,-109.5 435.5,-73.5 307.5,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"371.5\" y=\"-87.8\">dense_5: Dense</text>\n",
       "</g>\n",
       "<!-- 139733401092280&#45;&gt;139733772990560 -->\n",
       "<g class=\"edge\" id=\"edge15\">\n",
       "<title>139733401092280-&gt;139733772990560</title>\n",
       "<path d=\"M371.5,-146.4551C371.5,-138.3828 371.5,-128.6764 371.5,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"375.0001,-119.5903 371.5,-109.5904 368.0001,-119.5904 375.0001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139733887790160 -->\n",
       "<g class=\"node\" id=\"node14\">\n",
       "<title>139733887790160</title>\n",
       "<polygon fill=\"none\" points=\"307.5,-.5 307.5,-36.5 435.5,-36.5 435.5,-.5 307.5,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"371.5\" y=\"-14.8\">dense_6: Dense</text>\n",
       "</g>\n",
       "<!-- 139733772990560&#45;&gt;139733887790160 -->\n",
       "<g class=\"edge\" id=\"edge16\">\n",
       "<title>139733772990560-&gt;139733887790160</title>\n",
       "<path d=\"M371.5,-73.4551C371.5,-65.3828 371.5,-55.6764 371.5,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"375.0001,-46.5903 371.5,-36.5904 368.0001,-46.5904 375.0001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'lm_classification_imdb'\n",
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 500\n",
    "FILE_PATH = \"data/training_model.hdf5\"\n",
    "checkpoint = ModelCheckpoint(FILE_PATH, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10)\n",
    "tensorboard = TensorBoard(log_dir='/home/ml-notebooks/models/tensorboard/{}'.format(MODEL_NAME), \n",
    "                          batch_size=BATCH_SIZE)\n",
    "callbacks_list = [checkpoint, early]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23750 samples, validate on 1250 samples\n",
      "Epoch 1/500\n",
      "23552/23750 [============================>.] - ETA: 0s - loss: 0.9598 - acc: 0.8051\n",
      "Epoch 00001: val_loss did not improve\n",
      "23750/23750 [==============================] - 21s 887us/step - loss: 0.9560 - acc: 0.8058 - val_loss: 0.5779 - val_acc: 0.8680\n",
      "Epoch 2/500\n",
      "23552/23750 [============================>.] - ETA: 0s - loss: 0.3826 - acc: 0.9151\n",
      "Epoch 00002: val_loss did not improve\n",
      "23750/23750 [==============================] - 19s 814us/step - loss: 0.3822 - acc: 0.9153 - val_loss: 0.4122 - val_acc: 0.9016\n",
      "Epoch 3/500\n",
      "23552/23750 [============================>.] - ETA: 0s - loss: 0.3129 - acc: 0.9431\n",
      "Epoch 00003: val_loss improved from 0.40375 to 0.39473, saving model to data/training_model.hdf5\n",
      "23750/23750 [==============================] - 21s 886us/step - loss: 0.3131 - acc: 0.9429 - val_loss: 0.3947 - val_acc: 0.9080\n",
      "Epoch 4/500\n",
      "23552/23750 [============================>.] - ETA: 0s - loss: 0.2926 - acc: 0.9497\n",
      "Epoch 00004: val_loss improved from 0.39473 to 0.39005, saving model to data/training_model.hdf5\n",
      "23750/23750 [==============================] - 20s 835us/step - loss: 0.2922 - acc: 0.9498 - val_loss: 0.3901 - val_acc: 0.9088\n",
      "Epoch 5/500\n",
      "23552/23750 [============================>.] - ETA: 0s - loss: 0.2826 - acc: 0.9543\n",
      "Epoch 00005: val_loss did not improve\n",
      "23750/23750 [==============================] - 19s 813us/step - loss: 0.2827 - acc: 0.9543 - val_loss: 0.3935 - val_acc: 0.9120\n",
      "Epoch 6/500\n",
      "23552/23750 [============================>.] - ETA: 0s - loss: 0.2729 - acc: 0.9555\n",
      "Epoch 00006: val_loss did not improve\n",
      "23750/23750 [==============================] - 20s 825us/step - loss: 0.2727 - acc: 0.9555 - val_loss: 0.3936 - val_acc: 0.9120\n",
      "Epoch 7/500\n",
      "23552/23750 [============================>.] - ETA: 0s - loss: 0.2670 - acc: 0.9573\n",
      "Epoch 00007: val_loss did not improve\n",
      "23750/23750 [==============================] - 20s 823us/step - loss: 0.2669 - acc: 0.9573 - val_loss: 0.4024 - val_acc: 0.9096\n",
      "Epoch 8/500\n",
      "23552/23750 [============================>.] - ETA: 0s - loss: 0.2569 - acc: 0.9611\n",
      "Epoch 00008: val_loss did not improve\n",
      "23750/23750 [==============================] - 20s 826us/step - loss: 0.2567 - acc: 0.9611 - val_loss: 0.3987 - val_acc: 0.9104\n",
      "Epoch 9/500\n",
      "23552/23750 [============================>.] - ETA: 0s - loss: 0.2536 - acc: 0.9609\n",
      "Epoch 00009: val_loss did not improve\n",
      "23750/23750 [==============================] - 20s 825us/step - loss: 0.2536 - acc: 0.9609 - val_loss: 0.4091 - val_acc: 0.9032\n",
      "Epoch 10/500\n",
      "23552/23750 [============================>.] - ETA: 0s - loss: 0.2502 - acc: 0.9623\n",
      "Epoch 00010: val_loss did not improve\n",
      "23750/23750 [==============================] - 20s 832us/step - loss: 0.2501 - acc: 0.9624 - val_loss: 0.4237 - val_acc: 0.9080\n",
      "Epoch 11/500\n",
      "23552/23750 [============================>.] - ETA: 0s - loss: 0.2466 - acc: 0.9620\n",
      "Epoch 00011: val_loss did not improve\n",
      "23750/23750 [==============================] - 19s 816us/step - loss: 0.2465 - acc: 0.9621 - val_loss: 0.4344 - val_acc: 0.9024\n",
      "Epoch 12/500\n",
      "23552/23750 [============================>.] - ETA: 0s - loss: 0.2492 - acc: 0.9614\n",
      "Epoch 00012: val_loss did not improve\n",
      "23750/23750 [==============================] - 19s 819us/step - loss: 0.2492 - acc: 0.9616 - val_loss: 0.4080 - val_acc: 0.9136\n",
      "Epoch 13/500\n",
      "23552/23750 [============================>.] - ETA: 0s - loss: 0.2427 - acc: 0.9628\n",
      "Epoch 00013: val_loss did not improve\n",
      "23750/23750 [==============================] - 20s 827us/step - loss: 0.2430 - acc: 0.9627 - val_loss: 0.4362 - val_acc: 0.9024\n",
      "Epoch 14/500\n",
      "23552/23750 [============================>.] - ETA: 0s - loss: 0.2487 - acc: 0.9592\n",
      "Epoch 00014: val_loss did not improve\n",
      "23750/23750 [==============================] - 20s 824us/step - loss: 0.2484 - acc: 0.9593 - val_loss: 0.4252 - val_acc: 0.9032\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f156e890da0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "          validation_data=[X_val, y_val],\n",
    "          callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model(FILE_PATH, custom_objects={'AttentionWithContext':AttentionWithContext})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model(FILE_PATH, custom_objects={'AttentionWithContext':AttentionWithContext})\n",
    "y_hat = model.predict(X_test)\n",
    "y_hat = y_hat[:,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.91232\n",
      "Confusion Matrix: \n",
      "[[11380  1120]\n",
      " [ 1072 11428]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.91      0.91     12500\n",
      "          1       0.91      0.91      0.91     12500\n",
      "\n",
      "avg / total       0.91      0.91      0.91     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {}'.format(accuracy_score(y_test[:,1], y_hat > 0.5)))\n",
    "print('Confusion Matrix: ')\n",
    "print(confusion_matrix(y_test[:,1], y_hat > 0.5))\n",
    "print(classification_report(y_test[:,1], y_hat > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x7f16578ac358>,\n",
       " <keras.layers.embeddings.Embedding at 0x7f16578ac3c8>,\n",
       " <keras.layers.core.Dropout at 0x7f16578ac4e0>,\n",
       " <keras.layers.convolutional.Conv1D at 0x7f16578ac438>,\n",
       " <keras.layers.convolutional.Conv1D at 0x7f16578ac518>,\n",
       " <keras.layers.convolutional.Conv1D at 0x7f16578ac6a0>,\n",
       " <keras.layers.merge.Concatenate at 0x7f16578aca20>,\n",
       " <keras.layers.wrappers.Bidirectional at 0x7f164eec1128>,\n",
       " <keras.layers.convolutional.Conv1D at 0x7f164eec14a8>,\n",
       " <__main__.AttentionWithContext at 0x7f1637bc4668>,\n",
       " <keras.layers.pooling.GlobalMaxPooling1D at 0x7f1637c49dd8>,\n",
       " <keras.layers.merge.Add at 0x7f1637bc40b8>,\n",
       " <keras.layers.core.Dense at 0x7f164de6f860>,\n",
       " <keras.layers.core.Dense at 0x7f1654beac50>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unfreeze and Retrain\n",
    "Once the model stops learning, we can unfreeze some of the layers and then retrain again.  This process works better than unfreezing everything at once. If you want to learn more check out the fast.ai blog and video on [ULMFiT](http://nlp.fast.ai/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in model.layers[-3]:\n",
    "    layer.trainable = True\n",
    "\n",
    "opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "          validation_data=[X_val, y_val],\n",
    "          callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Attention\n",
    "\n",
    "This is a custom Keras layer for Attention on an LSTM.  It's adapted from a gist by [cbaziotis](https://gist.github.com/cbaziotis/6428df359af27d58078ca5ed9792bd6d), and designed to create a single feature vector from a seqeuence.  You'll have to load this cell before you can use the AttentionWithContext layer in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "    \n",
    "\n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number  to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
